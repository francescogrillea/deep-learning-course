{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/07/78/c23e1c70b89f361d855a5d0a19b229297f6456961f9a1afa9a69cd5a70c3/transformers-4.41.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/f4/85/d999b9a05fd101d48f1a365d68be0b109277bb25c89fb37a389d669f9185/tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\pycharm\\deep-learning-lab\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/9.1 MB 4.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/9.1 MB 4.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.7/9.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.0/9.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.7/9.1 MB 6.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.1 MB 6.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/9.1 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.7/9.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.0/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.2/9.1 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/9.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.2/9.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.5/9.1 MB 6.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.8/9.1 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.8/9.1 MB 6.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.9/9.1 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.8/9.1 MB 6.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.2/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.5/9.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.7/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.7/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.6/9.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 6.7 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T14:52:34.009687Z",
     "start_time": "2024-05-18T14:52:00.562414Z"
    }
   },
   "id": "79b6f91310f9d4e3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2be7a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T14:53:44.898888Z",
     "start_time": "2024-05-18T14:53:44.693974Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nStableDiffusionPipeline requires the transformers library but it was not found in your environment. You can install it with pip: `pip\ninstall transformers`\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstabilityai/stable-diffusion-2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m EulerDiscreteScheduler\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id, subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscheduler\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m pipe \u001B[38;5;241m=\u001B[39m \u001B[43mStableDiffusionPipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# pipe = pipe.to(\"cuda\")\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Projects\\PyCharm\\deep-learning-lab\\.venv\\lib\\site-packages\\diffusers\\utils\\dummy_torch_and_transformers_objects.py:1127\u001B[0m, in \u001B[0;36mStableDiffusionPipeline.from_pretrained\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1125\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_pretrained\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m-> 1127\u001B[0m     \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtransformers\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Projects\\PyCharm\\deep-learning-lab\\.venv\\lib\\site-packages\\diffusers\\utils\\import_utils.py:535\u001B[0m, in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m    533\u001B[0m failed \u001B[38;5;241m=\u001B[39m [msg\u001B[38;5;241m.\u001B[39mformat(name) \u001B[38;5;28;01mfor\u001B[39;00m available, msg \u001B[38;5;129;01min\u001B[39;00m checks \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m available()]\n\u001B[0;32m    534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failed:\n\u001B[1;32m--> 535\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(failed))\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m [\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVersatileDiffusionTextToImagePipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    539\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVersatileDiffusionPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnCLIPPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    543\u001B[0m ] \u001B[38;5;129;01mand\u001B[39;00m is_transformers_version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4.25.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    544\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m    545\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou need to install `transformers>=4.25` in order to use \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m```\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m pip install\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    546\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m --upgrade transformers \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m```\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    547\u001B[0m     )\n",
      "\u001B[1;31mImportError\u001B[0m: \nStableDiffusionPipeline requires the transformers library but it was not found in your environment. You can install it with pip: `pip\ninstall transformers`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2\"\n",
    "\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a278aa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T14:51:48.766955Z",
     "start_time": "2024-05-18T14:51:48.765952Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"oil painting of the Leaning Tower of Pisa at night with the moon\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"night.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"oil painting of the Leaning Tower of Pisa during the sunset\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"sunset.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae802768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_image = Image.open(\"night.png\")\n",
    "    start_image = TF.resize(start_image, (256, 256))\n",
    "    start_image = TF.to_tensor(start_image)\n",
    "    start_image = start_image.unsqueeze(0).half().cuda()\n",
    "\n",
    "    end_image = Image.open(\"sunset.png\")\n",
    "    end_image = TF.resize(end_image, (256, 256))\n",
    "    end_image = TF.to_tensor(end_image)\n",
    "    end_image = end_image.unsqueeze(0).half().cuda()\n",
    "\n",
    "\n",
    "    start_latent_space = pipe.vae.encode(start_image)\n",
    "    start_sample = start_latent_space.latent_dist.mean\n",
    "\n",
    "    end_latent_space = pipe.vae.encode(end_image)\n",
    "    end_sample = end_latent_space.latent_dist.mean\n",
    "\n",
    "    direction = end_sample - start_sample\n",
    "\n",
    "    frames = []\n",
    "    steps = 150\n",
    "    for i in trange(0, steps+1):\n",
    "        sample = start_sample + direction * (i / steps)\n",
    "        out = pipe.vae.decode(sample)\n",
    "        out = out.sample[0].float().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        out = (out * 255).astype(\"uint8\")\n",
    "        frames.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aae653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.mimsave('morph.gif', frames, fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
